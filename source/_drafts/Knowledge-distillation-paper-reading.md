---
layout: draft
title: Knowledge distillation paper reading
date: 2024-10-04 12:26:49
tags:
---

🔍 Geoffrey Hinton, Oriol Vinyals, Jeff Dean. "Distilling the Knowledge in a Neural Network" (Deep Learning and Representation Learning Workshop: NeurIPS 2014)
🔍 Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, Yoshua Bengio. "FitNets: Hints for Thin Deep Nets" (ICLR 2015)
🔍 Junho Yim, Donggyu Joo, Jihoon Bae, Junmo Kim. "A Gift From Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning" (CVPR 2017)
🔍 Sergey Zagoruyko, Nikos Komodakis. "Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer" (ICLR 2017)
🔍 Nikolaos Passalis, Anastasios Tefas. "Learning Deep Representations with Probabilistic Knowledge Transfer" (ECCV 2018)
🔍 Jangho Kim, Seonguk Park, Nojun Kwak. "Paraphrasing Complex Network: Network Compression via Factor Transfer" (NeurIPS 2018)
🔍 Byeongho Heo, Minsik Lee, Sangdoo Yun, Jin Young Choi. "Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons" (AAAI 2019)
🔍 Tong He, Chunhua Shen, Zhi Tian, Dong Gong, Changming Sun, Youliang Yan. "Knowledge Adaptation for Efficient Semantic Segmentation" (CVPR 2019)
🔍 Wonpyo Park, Dongju Kim, Yan Lu, Minsu Cho. "Relational Knowledge Distillation" (CVPR 2019)
🔍 Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D. Lawrence, Zhenwen Dai. "Variational Information Distillation for Knowledge Transfer" (CVPR 2019)
🔍 Yoshitomo Matsubara, Sabur Baidya, Davide Callegaro, Marco Levorato, Sameer Singh. "Distilled Split Deep Neural Networks for Edge-Assisted Real-Time Systems" (Workshop on Hot Topics in Video Analytics and Intelligent Edges: MobiCom 2019)
🔍 Baoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li, Yichao Wu, Yu Liu, Shunfeng Zhou, Zhaoning Zhang. "Correlation Congruence for Knowledge Distillation" (ICCV 2019)
🔍 Frederick Tung, Greg Mori. "Similarity-Preserving Knowledge Distillation" (ICCV 2019)
🔍 Yonglong Tian, Dilip Krishnan, Phillip Isola. "Contrastive Representation Distillation" (ICLR 2020)
🔍 Yoshitomo Matsubara, Marco Levorato. "Neural Compression and Filtering for Edge-assisted Real-time Object Detection in Challenged Networks" (ICPR 2020)
🔍 Li Yuan, Francis E.H.Tay, Guilin Li, Tao Wang, Jiashi Feng. "Revisiting Knowledge Distillation via Label Smoothing Regularization" (CVPR 2020)
🔍 Guodong Xu, Ziwei Liu, Xiaoxiao Li, Chen Change Loy. "Knowledge Distillation Meets Self-Supervision" (ECCV 2020)
🔍 Youcai Zhang, Zhonghao Lan, Yuchen Dai, Fangao Zeng, Yan Bai, Jie Chang, Yichen Wei. "Prime-Aware Adaptive Distillation" (ECCV 2020)
🔍 Pengguang Chen, Shu Liu, Hengshuang Zhao, Jiaya Jia. "Distilling Knowledge via Knowledge Review" (CVPR 2021)
🔍 Li Liu, Qingle Huang, Sihao Lin, Hongwei Xie, Bing Wang, Xiaojun Chang, Xiaodan Liang. "Exploring Inter-Channel Correlation for Diversity-Preserved Knowledge Distillation" (ICCV 2021)
🔍 Tao Huang, Shan You, Fei Wang, Chen Qian, Chang Xu. "Knowledge Distillation from A Stronger Teacher" (NeurIPS 2022)
🔍 Roy Miles, Krystian Mikolajczyk. "Understanding the Role of the Projector in Knowledge Distillation" (AAAI 2024)
🔍 Shangquan Sun, Wenqi Ren, Jingzhi Li, Rui Wang, Xiaochun Cao. "Logit Standardization in Knowledge Distillation" (CVPR 2024)